{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from bayes_opt import BayesianOptimization\n",
    "#bayesian-optimization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Measure Performance\n",
    "from  sklearn  import  metrics\n",
    "def measure_performance(X,y,clf, show_accuracy=True, show_classification_report=True, show_confusion_matrix=True, show_roc_auc = True):\n",
    "    y_pred = clf.predict(X)\n",
    "    y_predprob = clf.predict_proba(X)[:,1]\n",
    "    if show_accuracy:\n",
    "        print (\"Accuracy:{0:.3f}\".format(metrics.accuracy_score(y,y_pred))),\"\\n\"\n",
    "\n",
    "    if show_classification_report:\n",
    "        print(\"Classification report\")\n",
    "        print(metrics.classification_report(y,y_pred)),\"\\n\"\n",
    "        \n",
    "    if show_confusion_matrix:\n",
    "        print(\"Confusion matrix\")\n",
    "        print(metrics.confusion_matrix(y,y_pred)),\"\\n\"  \n",
    "        \n",
    "    if show_roc_auc:\n",
    "        print(\"ROC AUC Score\")\n",
    "        print(metrics.roc_auc_score(y,y_predprob)),\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparation for LightGBM\n",
    "import os\n",
    "application_train = pd.read_csv('/Users/francislin/Desktop/Machine_Learning_Workshop/Home Credit Default Risk/application_train.csv')\n",
    "\n",
    "# use LabelEncoder to convert categorical features to int type before construct Dataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def label_encoder(input_df, encoder_dict=None):\n",
    "    \"\"\" Process a dataframe into a form useable by LightGBM \"\"\"\n",
    "    # Label encode categoricals\n",
    "    categorical_feats = input_df.columns[input_df.dtypes == 'object']\n",
    "    for feat in categorical_feats:\n",
    "        encoder = LabelEncoder()\n",
    "        input_df[feat] = encoder.fit_transform(input_df[feat].fillna('NULL'))\n",
    "    return input_df, categorical_feats.tolist(), encoder_dict\n",
    "application_train, categorical_feats, encoder_dict = label_encoder(application_train)\n",
    "X = application_train.drop('TARGET', axis=1)\n",
    "y = application_train.TARGET\n",
    "\n",
    "# Prepare dataset \n",
    "seed = 7\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayesian-optimization\n",
    "def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n",
    "    params = {'application':'binary','num_iterations':4000, 'learning_rate':0.05, 'early_stopping_round':100, 'metric':'auc'}\n",
    "    params[\"num_leaves\"] = int(round(num_leaves))\n",
    "    params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "    params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "    params['max_depth'] = round(max_depth)\n",
    "    params['lambda_l1'] = max(lambda_l1, 0)\n",
    "    params['lambda_l2'] = max(lambda_l2, 0)\n",
    "    params['min_split_gain'] = min_split_gain\n",
    "    params['min_child_weight'] = min_child_weight\n",
    "    cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n",
    "    return max(cv_result['auc-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Set the range for each parameter (make the range as narrow as possible)\n",
    "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n",
    "                                        'feature_fraction': (0.1, 0.9),\n",
    "                                        'bagging_fraction': (0.8, 1),\n",
    "                                        'max_depth': (5, 8.99),\n",
    "                                        'lambda_l1': (0, 5),\n",
    "                                        'lambda_l2': (0, 3),\n",
    "                                        'min_split_gain': (0.001, 0.1),\n",
    "                                        'min_child_weight': (5, 50)}, random_state=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
